{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIE-Project B || Group - 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.signal import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# from sklearn import preprocessing ##importing for normalization\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MinMaxScaler, minmax_scale, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, LSTM, CuDNNLSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['legend.frameon'] = True\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the root directory and change to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = os.getcwd()\n",
    "os.chdir(rootdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Folder = 'EPOT_DATA'\n",
    "Validation_Aug_Folder = 'Validation_augmented_data'\n",
    "Experiment = 'Experiment'\n",
    "\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parse the training and validation-augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for training and validation\n",
    "training_tr = []; training_tr_labels = []\n",
    "\n",
    "validation_aug = []; validation_aug_labels = [] # validation augmented data\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    \n",
    "    if Training_Folder.lower() in subdir.lower():\n",
    "        for file in files:\n",
    "            if file.endswith('.mat'):\n",
    "                lbl = [file.split('_')[1], file.split('_')[2][:3]]\n",
    "                lb = np.asarray(lbl, dtype=float)\n",
    "                # load mat-file\n",
    "                mat = loadmat(os.path.join(subdir, file))\n",
    "                # get the numpy data from the mat file\n",
    "                mat = mat['num_data']\n",
    "                # append data to the list\n",
    "                training_tr.append(mat)\n",
    "                training_tr_labels.append(lb)\n",
    "\n",
    "    elif Validation_Aug_Folder.lower() in subdir.lower():\n",
    "        for file in files:\n",
    "            if file.endswith('.mat'):\n",
    "                lbl = [file.split('_')[1], file.split('_')[2][:3]]\n",
    "                lb = np.asarray(lbl, dtype=float)\n",
    "                # load mat-file\n",
    "                mat = loadmat(os.path.join(subdir, file))\n",
    "                # get the numpy data from the mat file\n",
    "                mat = mat['num_data']\n",
    "                # append data to the list\n",
    "                validation_aug.append(mat)\n",
    "                validation_aug_labels.append(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parse the experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for training and validation\n",
    "ex_data = []; ex_labels = []\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    \n",
    "    if Experiment.lower() in subdir.lower():\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                \n",
    "                with open(os.path.join(subdir, file), 'r') as f:\n",
    "                    x = pd.read_csv(os.path.join(subdir, file), delimiter='\\t', decimal=',', skiprows=8, header=None)\n",
    "                    x = np.asarray(x, dtype=float)\n",
    "                    ex_data.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "l = 3\n",
    "\n",
    "fig.suptitle(f'EPOS Training Data - {training_tr_labels[l]}')\n",
    "\n",
    "axs[0,0].plot(training_tr[l][:,0], training_tr[l][:,1], label='P - 1', c = 'r')\n",
    "axs[0,1].plot(training_tr[l][:,0], training_tr[l][:,2], label='P - 2', c = 'g')\n",
    "axs[1,0].plot(training_tr[l][:,0], training_tr[l][:,3], label='P - 3', c = 'b')\n",
    "axs[1,1].plot(training_tr[l][:,0], training_tr[l][:,4], label='P - 4', c = 'y')\n",
    "\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "l = 1\n",
    "plt.plot(ex_data[l][:,0], ex_data[l][:,1], label='P - 1', c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variable initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -----------------------------------------------------------------------------\n",
    "#Assuming P1 on top and counter clockwise order\n",
    "## -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Variables for the augmented data\n",
    "training_tl = np.zeros_like(np.asarray(training_tr)); training_tl_labels = np.zeros_like(np.asarray(training_tr_labels))\n",
    "training_bl = np.zeros_like(np.asarray(training_tr)); training_bl_labels = np.zeros_like(np.asarray(training_tr_labels))\n",
    "training_br = np.zeros_like(np.asarray(training_tr)); training_br_labels = np.zeros_like(np.asarray(training_tr_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top-Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(training_tr)):\n",
    "        training_tl[i][:,0] = training_tr[i][:,0]\n",
    "        training_tl[i][:,1] = training_tr[i][:,1]\n",
    "        training_tl[i][:,2] = training_tr[i][:,4]\n",
    "        training_tl[i][:,3] = training_tr[i][:,3]\n",
    "        training_tl[i][:,4] = training_tr[i][:,2]\n",
    "\n",
    "for i in range (0, len(training_tl_labels)):\n",
    "    training_tl_labels[i][0] = 500 - training_tr_labels[i][0]\n",
    "    training_tl_labels[i][1] = training_tr_labels[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bottom-Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(training_bl)):\n",
    "    if i in np.where(np.asanyarray(training_tl_labels) == 250.)[0]:\n",
    "        training_bl[i][:,0] = training_tl[i][:,0]\n",
    "        training_bl[i][:,1] = training_tl[i][:,1]\n",
    "        training_bl[i][:,2] = training_tl[i][:,2]\n",
    "        training_bl[i][:,3] = training_tl[i][:,3]\n",
    "        training_bl[i][:,4] = training_tl[i][:,4]\n",
    "    else:\n",
    "        training_bl[i][:,0] = training_tl[i][:,0]\n",
    "        training_bl[i][:,1] = training_tl[i][:,3]\n",
    "        training_bl[i][:,2] = training_tl[i][:,2]\n",
    "        training_bl[i][:,3] = training_tl[i][:,1]\n",
    "        training_bl[i][:,4] = training_tl[i][:,4]\n",
    "\n",
    "for i in range (0, len(training_bl_labels)):\n",
    "    training_bl_labels[i][0] = 500. - training_tr_labels[i][0]\n",
    "    training_bl_labels[i][1] = 500. - training_tr_labels[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bottom-Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(training_br)):\n",
    "        training_br[i][:,0] = training_bl[i][:,0]\n",
    "        training_br[i][:,1] = training_bl[i][:,1]\n",
    "        training_br[i][:,2] = training_bl[i][:,4]\n",
    "        training_br[i][:,3] = training_bl[i][:,3]\n",
    "        training_br[i][:,4] = training_bl[i][:,2]\n",
    "\n",
    "for i in range (0, len(training_br_labels)):\n",
    "    training_br_labels[i][0] = training_tr_labels[i][0]\n",
    "    training_br_labels[i][1] = 500 - training_tr_labels[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete overlapping data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tl = np.delete(np.asarray(training_tl), np.where(np.asanyarray(training_tl_labels) == 250.)[0], axis = 0)\n",
    "training_tl_labels = np.delete(np.asarray(training_tl_labels), np.where(np.asanyarray(training_tl_labels) == 250.)[0], axis = 0)\n",
    "\n",
    "training_bl = np.delete(np.asarray(training_bl), 0, axis = 0)\n",
    "training_bl_labels = np.delete(np.asarray(training_bl_labels), 0, axis = 0)\n",
    "\n",
    "training_br = np.delete(np.asarray(training_br), np.where(np.asanyarray(training_br_labels) == 250.)[0], axis = 0)\n",
    "training_br_labels = np.delete(np.asarray(training_br_labels), np.where(np.asanyarray(training_br_labels) == 250.)[0], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Makesure the augmentation is done on all quadrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.asarray(training_tl).shape} \\t{np.asarray(training_tr).shape} \\n{np.asarray(training_bl).shape} \\t{np.asarray(training_br).shape} \\n{counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "for i in range(0, len(training_tr_labels)):\n",
    "    plt.plot(training_tr_labels[i][0], training_tr_labels[i][1], 'o', c = 'r')\n",
    "for j in range (0, len(training_tl_labels)):\n",
    "    plt.plot(training_tl_labels[j][0], training_tl_labels[j][1], 's', c = 'g')\n",
    "    plt.plot(training_br_labels[j][0], training_br_labels[j][1], 'P', c = 'y')\n",
    "for k in range (0, len(training_bl_labels)):\n",
    "    plt.plot(training_bl_labels[k][0], training_bl_labels[k][1], '*', c = 'b')            \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = np.concatenate((training_tr, training_tl, training_bl, training_br), axis = 0)\n",
    "training_labels = np.concatenate((training_tr_labels, training_tl_labels, training_bl_labels, training_br_labels), axis = 0)\n",
    "\n",
    "print(f'Shape of all training set: {training.shape} \\nShape of all training labels: {training_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check with the validation augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig, axs = plt.subplots(2, 4, sharex=True)\n",
    "\n",
    "val_c = 1 # Given validation folder has 4 data points (0, 1, 2, 3)\n",
    "\n",
    "# find out the index of the validation set in the training data\n",
    "x = []\n",
    "for i in range (0, len(training_labels)):\n",
    "    if (training_labels[i][0] == validation_aug_labels[val_c][0]) and (training_labels[i][1] == validation_aug_labels[val_c][1]):\n",
    "        x.append(i)\n",
    "# plot to compare the validation set with the training set\n",
    "fig.suptitle(f'EPOS Training Data Coordinates - {training_labels[x[0]]} \\nValidation Augmented Data Coordinates - {validation_aug_labels[val_c]}')\n",
    "\n",
    "axs[0,0].plot(training[x[0]][:,0], training[x[0]][:,1], label='P - 1', c = 'r')\n",
    "axs[0,1].plot(training[x[0]][:,0], training[x[0]][:,2], label='P - 2', c = 'g')\n",
    "axs[0,2].plot(training[x[0]][:,0], training[x[0]][:,3], label='P - 3', c = 'b')\n",
    "axs[0,3].plot(training[x[0]][:,0], training[x[0]][:,4], label='P - 4', c = 'y')\n",
    "\n",
    "axs[1,0].plot(validation_aug[val_c][:,0], validation_aug[val_c][:,1], label='P - 1', c = 'r')\n",
    "axs[1,1].plot(validation_aug[val_c][:,0], validation_aug[val_c][:,2], label='P - 2', c = 'g')\n",
    "axs[1,2].plot(validation_aug[val_c][:,0], validation_aug[val_c][:,3], label='P - 3', c = 'b')\n",
    "axs[1,3].plot(validation_aug[val_c][:,0], validation_aug[val_c][:,4], label='P - 4', c = 'y')\n",
    "\n",
    "# check if the data is exactly the same\n",
    "for i in range (1, 5):\n",
    "    if np.all(training[x[0]][:,i] == validation_aug[val_c][:,i], axis = 0):\n",
    "        print('Data is augmented correctly')\n",
    "\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = training[x[0]][:,1] - validation_aug[val_c][:,1]\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(training)):\n",
    "    for j in range (1, 5):\n",
    "        training[i][:,j] = minmax_scale(training[i][:,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_resample = []\n",
    "# for i in range (0, len(training)):\n",
    "#     train_resample.append(pd.DataFrame(training[i], columns = ['time', '1', '2', '3', '4']))\n",
    "\n",
    "# size = 500\n",
    "\n",
    "# for i in range (0, len(training)):\n",
    "#     train_resample[i] = resample(train_resample[i], size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close()\n",
    "# plt.plot(training[0][:,0], training[0][:,1], c = 'r')\n",
    "# plt.plot(train_resample[0][:,0], train_resample[0][:,1], c = 'b')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0.2\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(training, training_labels, shuffle=True, test_size=size)\n",
    "\n",
    "print(f'Shape of training data - {x_train.shape} \\tShape of its labels - {y_train.shape} \\nShape of testing data - {x_test.shape} \\tShape of its labels - {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(CuDNNLSTM(units=128, input_shape=(x_train.shape))\n",
    "model.add(CuDNNLSTM(units=128)\n",
    "\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcdd480fa988c0bff1942a543d37a5a4859f7f33e82c1f9135f65e6471dc1f87"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
